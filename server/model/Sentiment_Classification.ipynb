{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sparshbohra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Congrats /r/anxiety we've all made it to Wedne...\n",
       "1    With both the subreddit and Discord continuing...\n",
       "2    I went to get my haircut and the person cuttin...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/reddit_data_2.0.csv').dropna()\n",
    "posts = df['body']\n",
    "posts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17568"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove punctuations\n",
    "2. Tokenization - Converting a sentence into list of words\n",
    "3. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>reddit_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Let your light shine!</td>\n",
       "      <td>19</td>\n",
       "      <td>qc0aqd</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qc0a...</td>\n",
       "      <td>25</td>\n",
       "      <td>Congrats /r/anxiety we've all made it to Wedne...</td>\n",
       "      <td>1.634735e+09</td>\n",
       "      <td>Congrats ranxiety weve all made it to Wednesda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Looking for new mods! (subreddit and Discord)</td>\n",
       "      <td>11</td>\n",
       "      <td>qb0ort</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qb0o...</td>\n",
       "      <td>0</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>1.634606e+09</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>fuck</td>\n",
       "      <td>409</td>\n",
       "      <td>qe7rl0</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qe7r...</td>\n",
       "      <td>47</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>1.635005e+09</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                          title  score      id  \\\n",
       "0  anxiety                          Let your light shine!     19  qc0aqd   \n",
       "1  anxiety  Looking for new mods! (subreddit and Discord)     11  qb0ort   \n",
       "2  anxiety                                           fuck    409  qe7rl0   \n",
       "\n",
       "  subreddit                                                url  num_comments  \\\n",
       "0   Anxiety  https://www.reddit.com/r/Anxiety/comments/qc0a...            25   \n",
       "1   Anxiety  https://www.reddit.com/r/Anxiety/comments/qb0o...             0   \n",
       "2   Anxiety  https://www.reddit.com/r/Anxiety/comments/qe7r...            47   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  Congrats /r/anxiety we've all made it to Wedne...  1.634735e+09   \n",
       "1  With both the subreddit and Discord continuing...  1.634606e+09   \n",
       "2  I went to get my haircut and the person cuttin...  1.635005e+09   \n",
       "\n",
       "                                        reddit_punct  \n",
       "0  Congrats ranxiety weve all made it to Wednesda...  \n",
       "1  With both the subreddit and Discord continuing...  \n",
       "2  I went to get my haircut and the person cuttin...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])     # this changes contraction to non-words (e.g. \"We've\" to \"weve\")\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "df['reddit_punct'] = df['body'].apply(lambda x: remove_punct(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>reddit_punct</th>\n",
       "      <th>reddit_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Let your light shine!</td>\n",
       "      <td>19</td>\n",
       "      <td>qc0aqd</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qc0a...</td>\n",
       "      <td>25</td>\n",
       "      <td>Congrats /r/anxiety we've all made it to Wedne...</td>\n",
       "      <td>1.634735e+09</td>\n",
       "      <td>Congrats ranxiety weve all made it to Wednesda...</td>\n",
       "      <td>[C, n, g, r,  , r, n, x, e,  , w, e, v, e,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Looking for new mods! (subreddit and Discord)</td>\n",
       "      <td>11</td>\n",
       "      <td>qb0ort</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qb0o...</td>\n",
       "      <td>0</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>1.634606e+09</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>[W, h,  , b, h,  , h, e,  , u, b, r, e,  , n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>fuck</td>\n",
       "      <td>409</td>\n",
       "      <td>qe7rl0</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qe7r...</td>\n",
       "      <td>47</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>1.635005e+09</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>[I,  , w, e, n,  ,  , g, e,  ,  , h, r, c, u, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                          title  score      id  \\\n",
       "0  anxiety                          Let your light shine!     19  qc0aqd   \n",
       "1  anxiety  Looking for new mods! (subreddit and Discord)     11  qb0ort   \n",
       "2  anxiety                                           fuck    409  qe7rl0   \n",
       "\n",
       "  subreddit                                                url  num_comments  \\\n",
       "0   Anxiety  https://www.reddit.com/r/Anxiety/comments/qc0a...            25   \n",
       "1   Anxiety  https://www.reddit.com/r/Anxiety/comments/qb0o...             0   \n",
       "2   Anxiety  https://www.reddit.com/r/Anxiety/comments/qe7r...            47   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  Congrats /r/anxiety we've all made it to Wedne...  1.634735e+09   \n",
       "1  With both the subreddit and Discord continuing...  1.634606e+09   \n",
       "2  I went to get my haircut and the person cuttin...  1.635005e+09   \n",
       "\n",
       "                                        reddit_punct  \\\n",
       "0  Congrats ranxiety weve all made it to Wednesda...   \n",
       "1  With both the subreddit and Discord continuing...   \n",
       "2  I went to get my haircut and the person cuttin...   \n",
       "\n",
       "                                      reddit_nonstop  \n",
       "0  [C, n, g, r,  , r, n, x, e,  , w, e, v, e,  , ...  \n",
       "1  [W, h,  , b, h,  , h, e,  , u, b, r, e,  , n, ...  \n",
       "2  [I,  , w, e, n,  ,  , g, e,  ,  , h, r, c, u, ...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['reddit_nonstop'] = df['reddit_punct'].apply(lambda x: remove_stopwords(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>reddit_punct</th>\n",
       "      <th>reddit_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Let your light shine!</td>\n",
       "      <td>19</td>\n",
       "      <td>qc0aqd</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qc0a...</td>\n",
       "      <td>25</td>\n",
       "      <td>Congrats /r/anxiety we've all made it to Wedne...</td>\n",
       "      <td>1.634735e+09</td>\n",
       "      <td>congrats ranxiety weve made wednesday this wee...</td>\n",
       "      <td>[C, n, g, r,  , r, n, x, e,  , w, e, v, e,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Looking for new mods! (subreddit and Discord)</td>\n",
       "      <td>11</td>\n",
       "      <td>qb0ort</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qb0o...</td>\n",
       "      <td>0</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>1.634606e+09</td>\n",
       "      <td>with subreddit discord continuing grow looking...</td>\n",
       "      <td>[W, h,  , b, h,  , h, e,  , u, b, r, e,  , n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>fuck</td>\n",
       "      <td>409</td>\n",
       "      <td>qe7rl0</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qe7r...</td>\n",
       "      <td>47</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>1.635005e+09</td>\n",
       "      <td>i went get haircut person cutting cut way shor...</td>\n",
       "      <td>[I,  , w, e, n,  ,  , g, e,  ,  , h, r, c, u, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Does anyone else feel bad for inanimate objects?</td>\n",
       "      <td>143</td>\n",
       "      <td>qeecds</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qeec...</td>\n",
       "      <td>27</td>\n",
       "      <td>For example, I feel bad for a snowblower that ...</td>\n",
       "      <td>1.635026e+09</td>\n",
       "      <td>for example i feel bad snowblower parents don’...</td>\n",
       "      <td>[F, r,  , e, x, p, l, e,  , I,  , f, e, e, l, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Does anybody have like a week of no anxiety th...</td>\n",
       "      <td>248</td>\n",
       "      <td>qe3u9l</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qe3u...</td>\n",
       "      <td>26</td>\n",
       "      <td>I swear I have been in a cycle of going “I’ve ...</td>\n",
       "      <td>1.634992e+09</td>\n",
       "      <td>i swear i cycle going “i’ve never felt better”...</td>\n",
       "      <td>[I,  , w, e, r,  , I,  , h, v, e,  , b, e, e, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                              title  score      id  \\\n",
       "0  anxiety                              Let your light shine!     19  qc0aqd   \n",
       "1  anxiety      Looking for new mods! (subreddit and Discord)     11  qb0ort   \n",
       "2  anxiety                                               fuck    409  qe7rl0   \n",
       "3  anxiety   Does anyone else feel bad for inanimate objects?    143  qeecds   \n",
       "4  anxiety  Does anybody have like a week of no anxiety th...    248  qe3u9l   \n",
       "\n",
       "  subreddit                                                url  num_comments  \\\n",
       "0   Anxiety  https://www.reddit.com/r/Anxiety/comments/qc0a...            25   \n",
       "1   Anxiety  https://www.reddit.com/r/Anxiety/comments/qb0o...             0   \n",
       "2   Anxiety  https://www.reddit.com/r/Anxiety/comments/qe7r...            47   \n",
       "3   Anxiety  https://www.reddit.com/r/Anxiety/comments/qeec...            27   \n",
       "4   Anxiety  https://www.reddit.com/r/Anxiety/comments/qe3u...            26   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  Congrats /r/anxiety we've all made it to Wedne...  1.634735e+09   \n",
       "1  With both the subreddit and Discord continuing...  1.634606e+09   \n",
       "2  I went to get my haircut and the person cuttin...  1.635005e+09   \n",
       "3  For example, I feel bad for a snowblower that ...  1.635026e+09   \n",
       "4  I swear I have been in a cycle of going “I’ve ...  1.634992e+09   \n",
       "\n",
       "                                        reddit_punct  \\\n",
       "0  congrats ranxiety weve made wednesday this wee...   \n",
       "1  with subreddit discord continuing grow looking...   \n",
       "2  i went get haircut person cutting cut way shor...   \n",
       "3  for example i feel bad snowblower parents don’...   \n",
       "4  i swear i cycle going “i’ve never felt better”...   \n",
       "\n",
       "                                      reddit_nonstop  \n",
       "0  [C, n, g, r,  , r, n, x, e,  , w, e, v, e,  , ...  \n",
       "1  [W, h,  , b, h,  , h, e,  , u, b, r, e,  , n, ...  \n",
       "2  [I,  , w, e, n,  ,  , g, e,  ,  , h, r, c, u, ...  \n",
       "3  [F, r,  , e, x, p, l, e,  , I,  , f, e, e, l, ...  \n",
       "4  [I,  , w, e, r,  , I,  , h, v, e,  , b, e, e, ...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def low_caps(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['reddit_punct'] = df['reddit_punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['reddit_punct'] = df['reddit_punct'].apply(lambda x: low_caps(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_data3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate polarity & subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(data):\n",
    "    #Create a function to get the subjectivity\n",
    "    def getSubjectivity(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    "    #Create a function to get the polarity\n",
    "    def getPolarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    #Create two new columns ‘Subjectivity’ & ‘Polarity’\n",
    "    data['TextBlob_Subjectivity'] = data['body'].apply(getSubjectivity)\n",
    "    data['TextBlob_Polarity'] = data['body'].apply(getPolarity)\n",
    "    def getAnalysis(score):\n",
    "        if score < 0:\n",
    "            return 'Negative'\n",
    "        elif score == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "    data['TextBlob_Analysis'] = data['TextBlob_Polarity'].apply(getAnalysis)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>reddit_punct</th>\n",
       "      <th>reddit_tokenized</th>\n",
       "      <th>reddit_nonstop</th>\n",
       "      <th>TextBlob_Subjectivity</th>\n",
       "      <th>TextBlob_Polarity</th>\n",
       "      <th>TextBlob_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Let your light shine!</td>\n",
       "      <td>19</td>\n",
       "      <td>qc0aqd</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qc0a...</td>\n",
       "      <td>25</td>\n",
       "      <td>Congrats /r/anxiety we've all made it to Wedne...</td>\n",
       "      <td>1.634735e+09</td>\n",
       "      <td>Congrats ranxiety weve all made it to Wednesda...</td>\n",
       "      <td>[congrats, ranxiety, weve, all, made, it, to, ...</td>\n",
       "      <td>[congrats, ranxiety, weve, made, wednesday, we...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>Looking for new mods! (subreddit and Discord)</td>\n",
       "      <td>11</td>\n",
       "      <td>qb0ort</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qb0o...</td>\n",
       "      <td>0</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>1.634606e+09</td>\n",
       "      <td>With both the subreddit and Discord continuing...</td>\n",
       "      <td>[with, both, the, subreddit, and, discord, con...</td>\n",
       "      <td>[subreddit, discord, continuing, grow, looking...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>fuck</td>\n",
       "      <td>409</td>\n",
       "      <td>qe7rl0</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>https://www.reddit.com/r/Anxiety/comments/qe7r...</td>\n",
       "      <td>47</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>1.635005e+09</td>\n",
       "      <td>I went to get my haircut and the person cuttin...</td>\n",
       "      <td>[i, went, to, get, my, haircut, and, the, pers...</td>\n",
       "      <td>[went, get, haircut, person, cutting, cut, way...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                          title  score      id  \\\n",
       "0  anxiety                          Let your light shine!     19  qc0aqd   \n",
       "1  anxiety  Looking for new mods! (subreddit and Discord)     11  qb0ort   \n",
       "2  anxiety                                           fuck    409  qe7rl0   \n",
       "\n",
       "  subreddit                                                url  num_comments  \\\n",
       "0   Anxiety  https://www.reddit.com/r/Anxiety/comments/qc0a...            25   \n",
       "1   Anxiety  https://www.reddit.com/r/Anxiety/comments/qb0o...             0   \n",
       "2   Anxiety  https://www.reddit.com/r/Anxiety/comments/qe7r...            47   \n",
       "\n",
       "                                                body       created  \\\n",
       "0  Congrats /r/anxiety we've all made it to Wedne...  1.634735e+09   \n",
       "1  With both the subreddit and Discord continuing...  1.634606e+09   \n",
       "2  I went to get my haircut and the person cuttin...  1.635005e+09   \n",
       "\n",
       "                                        reddit_punct  \\\n",
       "0  Congrats ranxiety weve all made it to Wednesda...   \n",
       "1  With both the subreddit and Discord continuing...   \n",
       "2  I went to get my haircut and the person cuttin...   \n",
       "\n",
       "                                    reddit_tokenized  \\\n",
       "0  [congrats, ranxiety, weve, all, made, it, to, ...   \n",
       "1  [with, both, the, subreddit, and, discord, con...   \n",
       "2  [i, went, to, get, my, haircut, and, the, pers...   \n",
       "\n",
       "                                      reddit_nonstop  TextBlob_Subjectivity  \\\n",
       "0  [congrats, ranxiety, weve, made, wednesday, we...                    0.0   \n",
       "1  [subreddit, discord, continuing, grow, looking...                    0.0   \n",
       "2  [went, get, haircut, person, cutting, cut, way...                    0.0   \n",
       "\n",
       "   TextBlob_Polarity TextBlob_Analysis  \n",
       "0                0.0           Neutral  \n",
       "1                0.0           Neutral  \n",
       "2                0.0           Neutral  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = sentiment_analysis(df)\n",
    "sentiment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative posts:  35\n",
      "percentage of negative posts:  1.7930327868852458 %\n"
     ]
    }
   ],
   "source": [
    "neg_sentiment = sentiment[sentiment['TextBlob_Analysis'] == 'Negative']  # range of polarity is [-1,1]\n",
    "print('number of negative posts: ',len(neg_sentiment))\n",
    "print('percentage of negative posts: ', len(neg_sentiment)/len(df)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjective posts:  73\n",
      "percentage of subjective posts:  3.7397540983606556 %\n"
     ]
    }
   ],
   "source": [
    "subj_sentiment = sentiment[sentiment['TextBlob_Subjectivity'] > 0.5] # range of subjectivity is [0,1]\n",
    "print('number of subjective posts: ',len(subj_sentiment))\n",
    "print('percentage of subjective posts: ', len(subj_sentiment)/len(df)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of negative & subjective posts:  35\n",
      "percentage of negative & subjective posts:  1.7930327868852458 %\n"
     ]
    }
   ],
   "source": [
    "new_data = sentiment[sentiment['TextBlob_Subjectivity'] > 0.5] \n",
    "new_data = new_data[new_data['TextBlob_Analysis']=='Negative']\n",
    "print('number of negative & subjective posts: ',len(new_data))\n",
    "print('percentage of negative & subjective posts: ', len(new_data)/len(df)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set:  24\n",
      "size of testing set:  11\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with 50%\n",
    "# values of original dataframe\n",
    "train = new_data.sample(frac = 0.7)\n",
    "train_label = train['subreddit']\n",
    "train_df = train['reddit_nonstop']\n",
    "  \n",
    "# Creating dataframe with \n",
    "# rest of the 50% values\n",
    "test = new_data.drop(train.index)\n",
    "test_label = test['subreddit']\n",
    "test_df = test['reddit_nonstop']\n",
    "print('size of training set: ',len(train))\n",
    "print('size of testing set: ',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, Input, Activation, Masking\n",
    "from tensorflow.python.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras import optimizers, initializers, layers\n",
    "\n",
    "# import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200\n",
    "tokenizer = Tokenizer(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "tokenizer.fit_on_texts(train_df)\n",
    "X_train_token = tokenizer.texts_to_sequences(train_df)\n",
    "\n",
    "tokenizer.fit_on_texts(test_df)\n",
    "X_test_token = tokenizer.texts_to_sequences(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 200) (179, 200)\n"
     ]
    }
   ],
   "source": [
    "# Pad\n",
    "\n",
    "X_train = pad_sequences(X_train_token, maxlen = maxlen, padding = 'post')\n",
    "X_test  = pad_sequences(X_test_token, maxlen = maxlen, padding = 'post')\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anxiety', 'Depression']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = ['Anxiety','Depression']\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 64)           49408     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 2,613,958\n",
      "Trainable params: 2,613,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Model\n",
    "\n",
    "model = Sequential([Input(shape=(maxlen, )),\n",
    "                    Embedding(max_features, 128, mask_zero = True),\n",
    "                    LSTM(64, return_sequences = True, dropout = 0.2),\n",
    "                    GlobalMaxPool1D(),\n",
    "                    Dropout(0.2),\n",
    "                    Dense(64, activation = 'relu'),\n",
    "                    Dropout(0.2),\n",
    "                    Dense(2, activation = 'softmax')])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a4b1d2c6b0b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     callbacks = [monitor])\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    495\u001b[0m                      'at same time.')\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m   \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0;31m# Handle validation_split, we want to split the data and get the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 653\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    654\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'str'>\"})"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "monitor = EarlyStopping(monitor = 'val_loss', \n",
    "                        min_delta = 1e-3, \n",
    "                        patience = 5, verbose = 1, \n",
    "                        restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train, targets,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 3, validation_split = 0.1,\n",
    "                    callbacks = [monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - keras\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    keras-2.3.1                |                0          12 KB\n",
      "    keras-base-2.3.1           |           py37_0         501 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         513 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  keras              pkgs/main/osx-64::keras-2.3.1-0\n",
      "  keras-base         pkgs/main/osx-64::keras-base-2.3.1-py37_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2021.10.~ --> pkgs/main::ca-certificates-2021.9.30-hecd8cb5_1\n",
      "  certifi            conda-forge::certifi-2021.10.8-py37hf~ --> pkgs/main::certifi-2021.10.8-py37hecd8cb5_0\n",
      "  conda              conda-forge::conda-4.10.3-py37hf98548~ --> pkgs/main::conda-4.10.3-py37hecd8cb5_0\n",
      "  openssl            conda-forge::openssl-1.1.1l-h0d85af4_0 --> pkgs/main::openssl-1.1.1l-h9ed2024_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "keras-base-2.3.1     | 501 KB    | ##################################### | 100% \n",
      "keras-2.3.1          | 12 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8e425231bc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:-<65} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in train_data:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# !pip install bert-tensorflow\n",
    "import bert\n",
    "# from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.0.0\n",
      "tensorflow_hub version :  0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version : \", tf.__version__)\n",
    "print(\"tensorflow_hub version : \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-309ac4ce0f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         vocab_file=vocab_file, do_lower_case=do_lower_case)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-309ac4ce0f98>\u001b[0m in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n\u001b[0;32m---> 11\u001b[0;31m                                             tokenization_info[\"do_lower_case\"]])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     return bert.tokenization.FullTokenizer(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1106\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 1000\n",
    "\n",
    "# Convert our train and validation features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_data, train_label, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "val_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524\n",
    "- https://towardsdatascience.com/cleaning-preprocessing-text-data-for-sentiment-analysis-382a41f150d6\n",
    "- https://github.com/HenrySilvaCS/SentiMentalHealth/blob/main/src/models.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
